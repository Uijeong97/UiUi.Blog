---
title: '[ì¸ê³µì§€ëŠ¥/ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ] ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì™„ë²½ ì •ë¦¬'
date: 2020-07-03 17:07:76
category: SoMa
thumbnail: './images/color.png'
tags: ["SoMa", "êµìœ¡"]
draft: false
---


# ëª©ì°¨
1. ì„ í˜•ëŒ€ìˆ˜í•™, Numpy (ìƒëµ)
2. [íšŒê·€ ë¶„ì„]()
3. [ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜]()
4. [K-Means í´ëŸ¬ìŠ¤í„°ë§]()


# 2. íšŒê·€ ë¶„ì„

## ğŸ“Œ **1. íšŒê·€ë¶„ì„ì´ë€?**

* ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì„ (í•¨ìˆ˜)ë¥¼ ì°¾ì•„ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì–´ë–¤ ê²°ê³¼ê°’ì„ ê°€ì§ˆì§€ ì˜ˆì¸¡í•˜ëŠ” ê²ƒ

## ğŸ“Œ **2. Loss Function**

* ë°ì´í„°ë¥¼ ì™„ë²½í•˜ê²Œ í‘œí˜„í•˜ëŠ” ì˜ˆì¸¡ì„ í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥
* ì „ì²´ ëª¨ë¸ì˜ ì°¨ì´ë¥¼ ìµœì†Œë¡œ í•˜ëŠ” Î²0, Î²1ì„ êµ¬í•˜ì

![loss function](./images/machine_learning/loss.png)

### ì‹¤ìŠµ

```python
def loss(x, y, beta_0, beta_1):
    N = len(x)
    
    '''
    x, y, beta_0, beta_1 ì„ ì´ìš©í•´ lossê°’ì„ ê³„ì‚°í•œ ë’¤ ë¦¬í„´í•©ë‹ˆë‹¤.
    '''
    x = np.array(x)
    y = np.array(y)
    return np.sum((y - (beta_0 * x + beta_1)) ** 2)
```

## ğŸ“Œ **3. ì‚° ì •ìƒ ì˜¤ë¥´ê¸°**

* ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” Î²0, Î²1ì„ êµ¬í•˜ê¸° ìœ„í•´, ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ìµœì†Œë¡œ ë§Œë“œëŠ” ê°’ì„ êµ¬í•´ì•¼í•¨. ì¦‰, Loss functionì„ ìµœì†Œë¡œ ë§Œë“œëŠ” Î²0, Î²1ì„ êµ¬í•´ì•¼í•¨.
* Loss functionì˜ ê²½ì‚¬ê°€ ê°€ì¥ ê°€íŒŒë¥´ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ì°¾ëŠ” ê²ƒì´ í•µì‹¬


### Scikit-learnì„ ì´ìš©í•œ íšŒê·€ë¶„ì„

```python
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

def loss(x, y, beta_0, beta_1):
    N = len(x)
    
    '''
    ì´ì „ ì‹¤ìŠµì—ì„œ êµ¬í˜„í•œ loss functionì„ ì—¬ê¸°ì— ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤.
    '''
    x = np.array(x)
    y = np.array(y)
    total_loss = np.sum((y - (beta_0 * x + beta_1) ** 2))

    return total_loss
    
X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

train_X = np.array(X).reshape(-1, 1)
train_Y = np.array(Y)

'''
ì—¬ê¸°ì—ì„œ ëª¨ë¸ì„ íŠ¸ë ˆì´ë‹í•©ë‹ˆë‹¤.
'''
lrmodel = LinearRegression()
lrmodel.fit(train_X, train_Y)

'''
lossê°€ ìµœì†Œê°€ ë˜ëŠ” ì§ì„ ì˜ ê¸°ìš¸ê¸°ì™€ ì ˆí¸ì„ ê³„ì‚°í•¨
'''
beta_0 = lrmodel.coef_[0]   # lrmodelë¡œ êµ¬í•œ ì§ì„ ì˜ ê¸°ìš¸ê¸°
beta_1 = lrmodel.intercept_ # lrmodelë¡œ êµ¬í•œ ì§ì„ ì˜ yì ˆí¸

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("Loss: %f" % loss(X, Y, beta_0, beta_1))

plt.scatter(X, Y) # (x, y) ì ì„ ê·¸ë¦½ë‹ˆë‹¤.
plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 ì— í•´ë‹¹í•˜ëŠ” ì„ ì„ ê·¸ë¦½ë‹ˆë‹¤.

plt.xlim(0, 10) # ê·¸ë˜í”„ì˜ Xì¶•ì„ ì„¤ì •í•©ë‹ˆë‹¤.
plt.ylim(0, 10) # ê·¸ë˜í”„ì˜ Yì¶•ì„ ì„¤ì •í•©ë‹ˆë‹¤.
```

## ğŸ“Œ **4. ë‹¤ì¤‘ì„ í˜•íšŒê·€ë¶„ì„(Multiple Linear Regression)**

* Featureê°€ ì—¬ëŸ¬ê°œ, outputì´ 1ê°œì¸ íšŒê·€ë¶„ì„
* Featureê°€ ì—¬ëŸ¬ê°œì¸ ì„ í˜•ì  ê´€ê³„ë¡œ ê°€ì •
* ê° ë°ì´í„°ì˜ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ ìµœì†Œí•œìœ¼ë¡œ ë§Œë“ ë‹¤.

![Multiple Linear Regression](./images/machine_learning/multi_loss.png)

### ì‹¤ìŠµ

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

'''
./data/Advertising.csv ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´, Xì™€ Yë¥¼ ë§Œë“­ë‹ˆë‹¤.

XëŠ” (200, 3) ì˜ shapeì„ ê°€ì§„ 2ì°¨ì› np.array,
YëŠ” (200,) ì˜ shapeì„ ê°€ì§„ 1ì°¨ì› np.arrayì—¬ì•¼ í•©ë‹ˆë‹¤.

XëŠ” FB, TV, Newspaper column ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
YëŠ” Sales column ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
'''

import csv
csvreader = csv.reader(open("data/Advertising.csv"))

x = []
y = []

next(csvreader)
for line in csvreader :
    x_i = [ float(line[1]), float(line[2]), float(line[3]) ]
    y_i = float(line[4])
    x.append(x_i)
    y.append(y_i)

X = np.array(x)
Y = np.array(y)

lrmodel = LinearRegression()
lrmodel.fit(X, Y)

beta_0 = lrmodel.coef_[0] # 0ë²ˆì§¸ ë³€ìˆ˜ì— ëŒ€í•œ ê³„ìˆ˜ (í˜ì´ìŠ¤ë¶)
beta_1 = lrmodel.coef_[1] # 1ë²ˆì§¸ ë³€ìˆ˜ì— ëŒ€í•œ ê³„ìˆ˜ (TV)
beta_2 = lrmodel.coef_[2] # 2ë²ˆì§¸ ë³€ìˆ˜ì— ëŒ€í•œ ê³„ìˆ˜ (ì‹ ë¬¸)
beta_3 = lrmodel.intercept_ # yì ˆí¸ (ê¸°ë³¸ íŒë§¤ëŸ‰)

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("beta_2: %f" % beta_2)
print("beta_3: %f" % beta_3)

def expected_sales(fb, tv, newspaper, beta_0, beta_1, beta_2, beta_3):
    '''
    FBì— fbë§Œí¼, TVì— tvë§Œí¼, Newspaperì— newspaper ë§Œí¼ì˜ ê´‘ê³ ë¹„ë¥¼ ì‚¬ìš©í–ˆê³ ,
    íŠ¸ë ˆì´ë‹ëœ ëª¨ë¸ì˜ weight ë“¤ì´ beta_0, beta_1, beta_2, beta_3 ì¼ ë•Œ
    ì˜ˆìƒë˜ëŠ” Sales ì˜ ì–‘ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
    '''
    result = beta_0 * fb + beta_1 * tv + beta_2 * newspaper + beta_3
    return result

print("ì˜ˆìƒ íŒë§¤ëŸ‰: %f" % expected_sales(10, 12, 3, beta_0, beta_1, beta_2, beta_3))
```

## ğŸ“Œ **5. ë‹¤í•­ì‹íšŒê·€ë¶„ì„(Polynomial Linear Regression)**

* ì„ í˜•ê´€ê³„ê°€ ì•„ë‹Œ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•¨


### ì‹¤ìŠµ1
```python
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

'''
./data/Advertising.csv ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´, Xì™€ Yë¥¼ ë§Œë“­ë‹ˆë‹¤.

XëŠ” (200, 3) ì˜ shapeì„ ê°€ì§„ 2ì°¨ì› np.array,
YëŠ” (200,) ì˜ shapeì„ ê°€ì§„ 1ì°¨ì› np.arrayì—¬ì•¼ í•©ë‹ˆë‹¤.

XëŠ” FB, TV, Newspaper column ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
YëŠ” Sales column ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
'''
data = pd.read_csv('data/Advertising.csv')
print(data.describe())
import csv
csvreader = csv.reader(open('data/Advertising.csv'))
x = []
y = []

next(csvreader)
for line in csvreader:
    x_i = [ float(line[1]), float(line[2]), float(line[3]) ]
    y_i = float(line[4])
    x.append(x_i)
    y.append(y_i)
    
X = np.array(x)
Y = np.array(y)

# ë‹¤í•­ì‹ íšŒê·€ë¶„ì„ì„ ì§„í–‰í•˜ê¸° ìœ„í•´ ë³€ìˆ˜ë“¤ì„ ì¡°í•©í•©ë‹ˆë‹¤.
X_poly = []
for x_i in X:
    X_poly.append([
        x_i[0], # X_1^2
        x_i[1], # X_2
        x_i[2], # X_3
        x_i[0] * x_i[1],
        x_i[0] * x_i[2],
        x_i[1] * x_i[2],
        x_i[2] ** 2,
        x_i[1] ** 2,
        x_i[0] ** 2
    ])

# X, Yë¥¼ 80:20ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. 80%ëŠ” íŠ¸ë ˆì´ë‹ ë°ì´í„°, 20%ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…ë‹ˆë‹¤.
x_train, x_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=0)

# x_train, y_trainì— ëŒ€í•´ ë‹¤í•­ì‹ íšŒê·€ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.
lrmodel = LinearRegression()
lrmodel.fit(x_train, y_train)

#x_trainì— ëŒ€í•´, ë§Œë“  íšŒê·€ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ êµ¬í•˜ê³ , ì´ ê°’ê³¼ y_train ì˜ ì°¨ì´ë¥¼ ì´ìš©í•´ MSEë¥¼ êµ¬í•©ë‹ˆë‹¤.
predicted_y_train = lrmodel.predict(x_train)
mse_train = mean_squared_error(y_train, predicted_y_train)
print("MSE on train data: {}".format(mse_train))

# x_testì— ëŒ€í•´, ë§Œë“  íšŒê·€ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ êµ¬í•˜ê³ , ì´ ê°’ê³¼ y_test ì˜ ì°¨ì´ë¥¼ ì´ìš©í•´ MSEë¥¼ êµ¬í•©ë‹ˆë‹¤. ì´ ê°’ì´ 1 ë¯¸ë§Œì´ ë˜ë„ë¡ ëª¨ë¸ì„ êµ¬ì„±í•´ ë´…ë‹ˆë‹¤.
predicted_y_test = lrmodel.predict(x_test)
mse_test = mean_squared_error(y_test, predicted_y_test)
print("MSE on test data: {}".format(mse_test))
```


### ë¯¸ì…˜2

```python
import operator
from sklearn.linear_model import LinearRegression
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import elice_utils


def main():
    words = read_data()
    words = sorted(words, key=lambda x: x[1], reverse=True) # words.txt ë‹¨ì–´ë¥¼ ë¹ˆë„ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
    # ì •ìˆ˜ë¡œ í‘œí˜„ëœ ë‹¨ì–´ë¥¼ Xì¶• ë¦¬ìŠ¤íŠ¸ì—, ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ Yì¶• ë¦¬ìŠ¤íŠ¸ì— ì €ì¥í•©ë‹ˆë‹¤.  
    X = list(range(1, len(words)+1))
    Y = [x[1] for x in words]
    
    # X, Y ë¦¬ìŠ¤íŠ¸ë¥¼ arrayë¡œ ë³€í™˜í•œ í›„ ê° ì›ì†Œ ê°’ì— log()ë¥¼ ì ìš©í•©ë‹ˆë‹¤.
    X, Y = np.array(X).reshape(-1, 1), np.array(Y).reshape(-1, 1)
    X, Y = np.log(X), np.log(Y)
    # ê¸°ìš¸ê¸°ì™€ ì ˆí¸ì„ êµ¬í•œ í›„ ê·¸ë˜í”„ì™€ ì°¨íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. 
    slope, intercept = do_linear_regression(X, Y)
    draw_chart(X, Y, slope, intercept)
    
    return slope, intercept


# read_data() - words.txtì— ì €ì¥ëœ ë‹¨ì–´ì™€ í•´ë‹¹ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ë¥¼ ë¦¬ìŠ¤íŠ¸í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
def read_data():
    # words.txt ì—ì„œ ë‹¨ì–´ë“¤ë¥¼ ì½ì–´, 
    # [[ë‹¨ì–´1, ë¹ˆë„ìˆ˜], [ë‹¨ì–´2, ë¹ˆë„ìˆ˜] ... ]í˜•ìœ¼ë¡œ ë³€í™˜í•´ ë¦¬í„´í•©ë‹ˆë‹¤.
    f = open('words.txt', 'r')
    words = []
    while True:
        line = f.readline()
        if not line: break
        word, num = line.strip().split(',')
        words.append([word, int(num)])
    f.close()
    return words


# do_linear_regression() - ì„í¬íŠ¸í•œ sklearn íŒ¨í‚¤ì§€ì˜ í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ê·¸ë˜í”„ì˜ ê¸°ìš¸ê¸°ì™€ ì ˆí¸ì„ êµ¬í•©ë‹ˆë‹¤.
def do_linear_regression(X, Y):
    # do_linear_regression() í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”. 
    lrmodel = LinearRegression()
    lrmodel.fit(X, Y)
    
    slope = lrmodel.coef_[0]
    intercept = lrmodel.intercept_
    
    return (slope, intercept)


# draw_chart() - matplotlibì„ ì´ìš©í•´ ì°¨íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
def draw_chart(X, Y, slope, intercept):
    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.scatter(X, Y)

    # ì°¨íŠ¸ì˜ X, Yì¶• ë²”ìœ„ì™€ ê·¸ë˜í”„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    min_X = min(X)
    max_X = max(X)
    min_Y = min_X * slope + intercept
    max_Y = max_X * slope + intercept
    plt.plot([min_X, max_X], [min_Y, max_Y], 
             color='red',
             linestyle='--',
             linewidth=3.0)
    
    # ê¸°ìš¸ê³¼ì™€ ì ˆí¸ì„ ì´ìš©í•´ ê·¸ë˜í”„ë¥¼ ì°¨íŠ¸ì— ì…ë ¥í•©ë‹ˆë‹¤.
    ax.text(min_X, min_Y + 0.1, r'$y = %.2lfx + %.2lf$' % (slope, intercept), fontsize=15)
    
    plt.savefig('chart.png')
    elice_utils.send_image('chart.png')

if __name__ == "__main__":
    main()
```


# 3. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜

## ğŸ“Œ **1. í™•ë¥ ê¸°ì´ˆ**

### í™•ë¥ ì´ë€
* ì–´ë–¤ ì‚¬ê±´ì´ ì¼ì–´ë‚  ê²ƒì´ì§€ í˜¹ì€ ì¼ì–´ë‚¬ëŠ”ì§€ì— ëŒ€í•œ ì§€ì‹ í˜¹ì€ ë¯¿ìŒì„ í‘œí˜„í•˜ëŠ” ë°©ë²•

## ğŸ“Œ **2. ë² ì´ì¦ˆ ë²•ì¹™**

```python
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np

import elice_utils

def main():
    plt.figure(figsize=(5,5))
    
    X = []
    Y = []
    
    # Nì„ 10ë°°ì”© ì¦ê°€í•  ë•Œ íŒŒì´ ê°’ì´ ì–´ë–»ê²Œ ë³€ê²½ë˜ëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”.
    N = 10000
    
    for i in range(N):
        X.append(np.random.rand() * 2 - 1)
        Y.append(np.random.rand() * 2 - 1)
    X = np.array(X)
    Y = np.array(Y)
    distance_from_zero = np.sqrt(X * X + Y * Y)
    is_inside_circle = distance_from_zero <= 1
    
    print("Estimated pi = %f" % (np.average(is_inside_circle) * 4))
    
    plt.scatter(X, Y, c=is_inside_circle)
    plt.savefig('circle.png')
    elice_utils.send_image('circle.png')

if __name__ == "__main__":
    main()
```

## ğŸ“Œ **3. ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜ê¸°**

### ë¹ˆë„ì£¼ì˜ì vs ë² ì´ì¦ˆì£¼ì˜ì

**ë¹ˆë„ì£¼ì˜ì**
* ë™ì „ì„ ìˆ˜ì²œ, ìˆ˜ë§Œ ë²ˆ ë˜ì¡Œì„ ë•Œ ê·¸ ì¤‘ ì•ë©´ì´ 50%, ë’·ë©´ì´ 50% ë‚˜ì˜¨ë‹¤.

**ë² ì´ì¦ˆì£¼ì˜ì**
* ë™ì „ ë˜ì§€ê¸°ì˜ ê²°ê³¼ê°€ ì•ë©´ì´ ë‚˜ì˜¬ ê²ƒì´ë¼ëŠ” ë¯¿ìŒì´ 50%ì´ë‹¤.

### ë² ì´ì¦ˆ ë²•ì¹™

![ë² ì´ì¦ˆ ë²•ì¹™](./images/machine_learning/base.png)

### ì˜ˆì œ: ì•” ê²€ì‚¬ í‚¤íŠ¸

```python
def main():
    sensitivity = float(input())
    prior_prob = float(input())
    false_alarm = float(input())

    print("%.2lf%%" % (100 * mammogram_test(sensitivity, prior_prob, false_alarm)))

def mammogram_test(sensitivity, prior_prob, false_alarm):
    p_a1_b1 = sensitivity # p(A = 1 | B = 1)

    p_b1 = prior_prob    # p(B = 1)

    p_b0 = 1 - prior_prob    # p(B = 0)

    p_a1_b0 = false_alarm # p(A = 1|B = 0)

    p_a1 = p_a1_b1 * p_b1 + p_a1_b0 * p_b0    # p(A = 1)

    p_b1_a1 = p_a1_b1 * p_b1 / p_a1 # p(B = 1|A = 1)

    return p_b1_a1

if __name__ == "__main__":
    main()
```

### ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜ê¸°

**Likelihood**
* prior probability P(A)ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, P(X|A)ë¥¼ likelihoodë¼ í•¨
* í…ŒìŠ¤íŠ¸í•˜ê³  ì‹¶ì€ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ í‘œí˜„í•˜ëŠ”ê°€ ë“±.

**prior probability**
* P(A)

**posterior probability**
* P(A|X)
* ë°ì´í„° Xê°€ ê´€ì°°ë˜ì—ˆì„ ë•Œ A í´ë˜ìŠ¤ì¼ í™•ë¥ 

**ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜**

![ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜](./images/machine_learning/naivebayes.png)


```python
import re
import math
import numpy as np

def main():
    M1 = {'r': 0.7, 'g': 0.2, 'b': 0.1} # M1 ê¸°ê³„ì˜ ì‚¬íƒ• ë¹„ìœ¨
    M2 = {'r': 0.3, 'g': 0.4, 'b': 0.3} # M2 ê¸°ê³„ì˜ ì‚¬íƒ• ë¹„ìœ¨
    
    test = {'r': 4, 'g': 3, 'b': 3}

    print(naive_bayes(M1, M2, test, 0.7, 0.3))

def naive_bayes(M1, M2, test, M1_prior, M2_prior):
    M1_posterior = M1_prior * (M1['r'] ** test['r']) * (M1['g'] ** test['g']) * (M1['b'] ** test['b'])
    M2_posterior = M2_prior * (M2['r'] ** test['r']) * (M2['g'] ** test['g'] * (M2['b'] ** test['b']))
    sum_M1M2 = M1_posterior + M2_posterior
    return [M1_posterior/sum_M1M2, M2_posterior/sum_M1M2]

if __name__ == "__main__":
    main()
```

## ğŸ“Œ **4. Bag of Wordsì™€ ê°ì„±ë¶„ì„**

### Bag of Words

* (ì „ì²˜ë¦¬) ìì—°ì–´ ë¬¸ì¥ì„ íŠ¹ìˆ˜ ë¬¸ìë¥¼ ì œê±°í•œ í›„, Tokenize
* (ê°œë…) ê°€ë°©ì•ˆì— ë‹¨ì–´ë¥¼ ë„£ê³  ë‹¨ì–´ì˜ ì¢…ë¥˜, ë¹ˆë„ë¥¼ ì •ë¦¬í•œ ê²ƒ
* ë‹¨ì–´ì˜ ìˆœì„œëŠ” ì¤‘ìš”í•˜ì§€ ì•Šê³ , ë‹¨ì–´ì˜ ë¹ˆë„ë§Œì„ ë”°ì§„ë‹¤.

```python
import re

special_chars_remover = re.compile("[^\w'|_]")

def main():
    sentence = input()
    bow = create_BOW(sentence)

    print(bow)

def create_BOW(sentence):
    bow = {}
    sentence = remove_special_characters(sentence)
    sentence = sentence.lower()
    for w in sentence.split():
        bow[w] = bow[w] + 1 if w in bow else 1
        
    return bow

def remove_special_characters(sentence):
    return special_chars_remover.sub(' ', sentence)


if __name__ == "__main__":
    main()
```

## ğŸ“Œ **5. ê°ì • ë¶„ë¥˜ê¸°**

* ê¸ì •ê¸°ê³„ì—ì„œ 'ìµœê³ ' ë‹¨ì–´ê°€ ë‚˜ì˜¬ í™•ë¥ 



# 4. K-Means í´ëŸ¬ìŠ¤í„°ë§

## ğŸ“Œ **1. ë¹„ì§€ë„í•™ìŠµ ê°œë¡ **

* ë‹µì´ ì •í•´ì ¸ ìˆì§€ ì•Šì€ ë°ì´í„°ì—ì„œ ìˆ¨ê²¨ì§„ êµ¬ì¡° íŒŒì•…

## ğŸ“Œ **2. Hard vs. Soft Clustering**

**Hard clustering**
* ë°ì´í„° í¬ì¸íŠ¸ë“¤ì€ ë¹„ìŠ·í•œ ê²ƒë“¤ë¼ë¦¬ ë­‰ì³ìˆë‹¤
* 100% ê°•ì•„ì§€, 0% ê³ ì–‘ì´ (ë¶„ë¥˜ì™€ ê°€ê¹Œì›€)
* K-Means, DBSCAN, OPTICS

**Soft clustering**
* í•œ ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ëŠ” ìˆ¨ê²¨ì§„ í´ëŸ¬ìŠ¤í„°ë“¤ì˜ ê²°í•©ì´ë‹¤.
* 60% ê³¼í•™, 35% íŒíƒ€ì§€, 5% ì—­ì‚¬
* Topic Models, FCM, Gaussian Mixture Models(EM), Soft K-Means

## ğŸ“Œ **3. K ê²°ì •í•˜ê¸°**

ì™„ë²½í•˜ê²Œ ê²°ì •í•˜ê¸°ëŠ” ì‰½ì§€ ì•ŠìŒ

**ê³ ë ¤í•  ê²ƒë“¤**
* ë°ì´í„°ì˜ íŠ¹ì„±
* ë¶„ì„ ê²°ê³¼ë¡œ ì–»ê³ ì í•˜ëŠ” ê²ƒ

## ğŸ“Œ **4. ì£¼ì„±ë¶„ ë¶„ì„(PCA)**

**ì‚¬ìš© ëª©ì **
* ê³ ì°¨ì›ì˜ ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì¤„ì´ê¸° ìœ„í•´ (ì˜ˆ: ì‹œê°í™”)
* ë°ì´í„° ì •ì œ

**ë°©ë²•**
1. í•œ columnì´ 0-1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë„ë¡ normalize

### ì‹¤ìŠµ

`sklearn.decomposition.PCA`

```python
import csv
import sklearn.decomposition
import matplotlib.pyplot as plt
import numpy as np
import elice_utils

def main():
    X, attributes = input_data()
    pca_array = normalize(X)
    pca, pca_array = run_PCA(X, 2)
    visualize_2d_wine(pca_array)

def input_data():
    X = []
    attributes = []
    with open("data/attributes.txt") as fp:
        attributes = fp.readlines()
    attributes = [x.strip() for x in attributes]
    csvreader = csv.reader(open("data/wine.csv"))
    for line in csvreader:
        float_number = [float(x) for x in line]
        X.append(float_number)
    return np.array(X), attributes

def run_PCA(X, num_components):
    pca = sklearn.decomposition.PCA(n_components = num_components)
    pca.fit(X)
    pca_array = pca.transform(X)
    return pca, pca_array
    
def normalize(X):
    '''
    ê°ê°ì˜ featureì— ëŒ€í•´,
    178ê°œì˜ ë°ì´í„°ì— ë‚˜íƒ€ë‚˜ëŠ” í•´ë‹¹í•˜ëŠ” featureì˜ ê°’ì´ ìµœì†Œ 0, ìµœëŒ€ 1ì´ ë˜ë„ë¡
    ì„ í˜•ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    '''
    for i in range(X.shape[1]):
        X[:, i] = X[:, i] - np.min(X[:, i])
        X[:, i] = X[:, i]/np.max(X[:, i])
    return X

def visualize_2d_wine(X):
    '''Xë¥¼ ì‹œê°í™”í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.'''
    plt.scatter(X[:, 0], X[:, 1])
    plt.savefig("image.png")
    elice_utils.send_image("image.png")

if __name__ == '__main__':
    main()
```

## ğŸ“Œ **5. í´ëŸ¬ìŠ¤í„°ë§**

* ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë¹„ìŠ·í•œ ê·¸ë£¹ (í´ëŸ¬ìŠ¤í„°) ìœ¼ë¡œ ë¬¶ëŠ” ì•Œê³ ë¦¬ì¦˜

## ğŸ“Œ **6. K-means**

* ë°˜ë³µì„ ì´ìš©í•œ í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜

1. Centroid : ê° í´ëŸ¬ìŠ¤í„°ì˜ ì¤‘ì‹¬ì„ ì˜ë¯¸
2. distance : ì¤‘ì‹¬ê³¼ ë°ì´í„° í¬ì¸íŠ¸ì™€ì˜ ê±°ë¦¬

**ìˆœì„œ**
1. ì„ì˜ë¡œ ì¤‘ì‹¬ì„ í•˜ë‚˜ ì •í•˜ê³  í´ëŸ¬ìŠ¤í„°ë§ ì§„í–‰
2. ì •í•´ì§„ í´ëŸ¬ìŠ¤í„°ì—ì„œ ì¤‘ì‹¬ì ì„ ë‹¤ì‹œ ê³„ì‚°
    - ì¤‘ì‹¬ì ì€ í•´ë‹¹ í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„° í¬ì¸í„° ìœ„ì¹˜ì˜ ë¬´ê²Œì¤‘ì‹¬ ê°’
3. ë‹¤ì‹œ í´ëŸ¬ìŠ¤í„°ë§, ì¤‘ì‹¬ ì—…ë°ì´íŠ¸ ì‹œ ë³€í•˜ì§€ ì•Šìœ¼ë©´ ê·¸ë§Œë‘ 

```python
import sklearn.decomposition
import sklearn.cluster
import matplotlib.pyplot as plt
import numpy as np
import elice_utils

def main():
    X, attributes = input_data()
    X = normalize(X)
    pca, pca_array = run_PCA(X, 2)
    labels = kmeans(pca_array, 3, [0, 1, 2])
    visualize_2d_wine(pca_array, labels)

def input_data():
    X = []
    attributes = []
    
    with open('data/wine.csv') as fp:
        for line in fp:
            X.append([float(x) for x in line.strip().split(',')])
    
    with open('data/attributes.txt') as fp:
        attributes = [x.strip() for x in fp.readlines()]

    return np.array(X), attributes

def run_PCA(X, num_components):
    pca = sklearn.decomposition.PCA(n_components=num_components)
    pca.fit(X)
    pca_array = pca.transform(X)

    return pca, pca_array

def kmeans(X, num_clusters, initial_centroid_indices):
    import time
    
    N = len(X)
    centroids = X[initial_centroid_indices]
    labels = np.zeros(N)

    while True:
        '''
        Step 1. ê° ë°ì´í„° í¬ì¸íŠ¸ i ì— ëŒ€í•´ ê°€ì¥ ê°€ê¹Œìš´
        ì¤‘ì‹¬ì ì„ ì°¾ê³ , ê·¸ ì¤‘ì‹¬ì ì— í•´ë‹¹í•˜ëŠ” í´ëŸ¬ìŠ¤í„°ë¥¼ í• ë‹¹í•˜ì—¬
        labels[i]ì— ë„£ìŠµë‹ˆë‹¤.
        ê°€ê¹Œìš´ ì¤‘ì‹¬ì ì„ ì°¾ì„ ë•ŒëŠ”, ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        ë¯¸ë¦¬ ì •ì˜ëœ distance í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        '''
        is_changed = False
        for i in range(N):
            distances = []
            for k in range(num_clusters):
                # X[i] ì™€ centeroids[k]
                k_dist = distance(X[i], centroids[k])
                distances.append(k_dist)
            if labels[i] != np.argmin(distances):
                is_changed = True
            labels[i] = np.argmin(distances)
        '''
        Step 2. í• ë‹¹ëœ í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ì¤‘ì‹¬ì ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        ì¤‘ì‹¬ì ì€ í´ëŸ¬ìŠ¤í„° ë‚´ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ ìœ„ì¹˜ì˜ *ì‚°ìˆ  í‰ê· *
        ìœ¼ë¡œ í•©ë‹ˆë‹¤.
        '''
        for k in range(num_clusters):
            x = X[labels == k][:, 0]
            y = X[labels == k][:, 1]
            
            x = np.mean(x)
            y = np.mean(y)
            centroids[k] = [x, y]
        
        '''
        Step 3. ë§Œì•½ í´ëŸ¬ìŠ¤í„°ì˜ í• ë‹¹ì´ ë°”ë€Œì§€ ì•Šì•˜ë‹¤ë©´ ì•Œê³ ë¦¬ì¦˜ì„ ëëƒ…ë‹ˆë‹¤.
        ì•„ë‹ˆë¼ë©´ ë‹¤ì‹œ ë°˜ë³µí•©ë‹ˆë‹¤.
        '''
        if not is_changed:
            break
            
    return labels

def distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))
    
def normalize(X):
    for dim in range(len(X[0])):
        X[:, dim] -= np.min(X[:, dim])
        X[:, dim] /= np.max(X[:, dim])
    return X

'''
ì´ì „ì— ë”í•´, ê°ê°ì˜ ë°ì´í„° í¬ì¸íŠ¸ì— ìƒ‰ì„ ì…íˆëŠ” ê³¼ì •ë„ ì§„í–‰í•©ë‹ˆë‹¤.
'''

def visualize_2d_wine(X, labels):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:, 0], X[:, 1], c=labels)
    plt.savefig("image.svg", format="svg")
    elice_utils.send_image("image.svg")

if __name__ == '__main__':
    main()
```

# 5. í¼ì…‰íŠ¸ë¡ 

ì‹ ê²½ë§ ì´ì „ì˜ ì—°êµ¬ëŠ”?
1. ì¶”ë¡ 
2. Heuristic
3. Lisp

## ğŸ“Œ **1. ì‹ ê²½ë§ì´ë€?**

**ë‰´ëŸ°**
* ë‘ë‡Œì˜ ê°€ì¥ ì‘ì€ ì •ë³´ì²˜ë¦¬ ë‹¨ìœ„

**ì¸ê³µ ì‹ ê²½ë§**
* feed forward network
* íšŒê·€ë¶„ì„, ë¶„ë¥˜, íŒ¨í„´íŒŒì•… ë“±ì— ì‚¬ìš©

## ğŸ“Œ **2. í¼ì…‰íŠ¸ë¡ **

* ë‰´ëŸ°ì„ ê·¸ëŒ€ë¡œ ì†Œí”„íŠ¸ì›¨ì–´ë¡œ ë§Œë“  ê²ƒ

### í¼ì…‰íŠ¸ë¡  ì‘ë™ ì˜ˆì‹œ

```python
from elice_utils import EliceUtils

elice_utils = EliceUtils()

# 1. ì‹ í˜¸ì˜ ì´í•©ê³¼ ì™¸ì¶œ ì—¬ë¶€ë¥¼ ë°˜í™˜í•˜ëŠ” Perceptron í•¨ìˆ˜ë¥¼ ì™„ì„±í•˜ì„¸ìš”.
def Perceptron(x_1,x_2,w_1,w_2):
    
    # biasëŠ” ì™¸ì¶œì„ ì¢‹ì•„í•˜ëŠ” ì •ë„ë¡œ -1ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    bias = -1
    
    # ì…ë ¥ ë°›ì€ ê°’ê³¼ í¸í–¥(bias)ê°’ì„ ì´ìš©í•˜ì—¬ ì‹ í˜¸ì˜ ì´í•©ì„ êµ¬í•˜ì„¸ìš”.
    output = w_1 * x_1 + w_2 * x_2 + bias
    
    # ì§€ì‹œí•œ Activation í•¨ìˆ˜ë¥¼ ì°¸ê³ í•˜ì—¬ ì™¸ì¶œ ì—¬ë¶€(0 or 1)ë¥¼ ì„¤ì •í•˜ì„¸ìš”.
    # ì™¸ì¶œ ì•ˆí•œë‹¤ : 0 / ì™¸ì¶œ í•œë‹¤ : 1 
    y = 1 if output > 0 else 0
    
    return output, y
    
# ê°’ì„ ì…ë ¥ ë°›ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. 
def input_func():
    
    # ë¹„ ì˜¤ëŠ” ì—¬ë¶€(ë¹„ê°€ ì˜¨ë‹¤ : 1 / ë¹„ê°€ ì˜¤ì§€ ì•ŠëŠ”ë‹¤ : 0)
    x_1 =  int(input("x_1 : ë¹„ê°€ ì˜¤ëŠ” ì—¬ë¶€(1 or 0)ì„ ì…ë ¥í•˜ì„¸ìš”."))
        
    # ì—¬ìì¹œêµ¬ê°€ ë§Œë‚˜ìê³  í•˜ëŠ” ì—¬ë¶€(ë§Œë‚˜ìê³  í•œë‹¤ : 1 / ë§Œë‚˜ìê³  í•˜ì§€ ì•ŠëŠ”ë‹¤ : 0)
    x_2 =  int(input("x_2 : ì—¬ì¹œì´ ë§Œë‚˜ìê³  í•˜ëŠ” ì—¬ë¶€(1 or 0)ì„ ì…ë ¥í•˜ì„¸ìš”."))
        
    # ë¹„ë¥¼ ì¢‹ì•„í•˜ëŠ” ì •ë„ì˜ ê°’(ë¹„ë¥¼ ì‹«ì–´í•œë‹¤ -5 ~ 5 ë¹„ë¥¼ ì¢‹ì•„í•œë‹¤)
    w_1 =  int(input("w_1 : ë¹„ë¥¼ ì¢‹ì•„í•˜ëŠ” ì •ë„ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”."))
        
    # ì—¬ìì¹œêµ¬ë¥¼ ì¢‹ì•„í•˜ëŠ” ì •ë„ì˜ ê°’(ì—¬ìì¹œêµ¬ë¥¼ ì‹«ì–´í•œë‹¤ -5 ~ 5 ë¹„ë¥¼ ì¢‹ì•„í•œë‹¤)
    w_2 =  int(input("w_2 : ì—¬ì¹œì„ ì¢‹ì•„í•˜ëŠ” ì •ë„ ê°’ì„ ì…ë ¥í•˜ì„¸ìš”."))
        
    return x_1,x_2,w_1,w_2
    
def main():
    
    x_1,x_2,w_1,w_2 = input_func()
    
    y, go_out = Perceptron(x_1,x_2,w_1,w_2)
    
    print("\nì‹ í˜¸ì˜ ì´í•© : %d" %y)
    print("ì™¸ì¶œ ì—¬ë¶€ : %d\n" %go_out)
    
if __name__ == "__main__":
    main()
```

## ğŸ“Œ **3. í¼ì…‰íŠ¸ë¡  ì„ í˜• ë¶„ë¥˜ê¸°**

* ë…¼ë¦¬ ì—°ì‚°ìë¥¼ Activationí•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©
* AND, OR, NAND, NOR ê²Œì´íŠ¸
* í¼ì…‰íŠ¸ë¡  ë…¼ë¦¬ íšŒë¡œì—ì„œ í™•ì¥ -> ì„ í˜• ë¶„ë¥˜ê¸°ë¡œ ì‘ë™

### ì‹¤ìŠµ-AND, OR

```python
from elice_utils import EliceUtils
elice_utils = EliceUtils()

import numpy as np

# 1. AND gate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
def AND_gate(x1, x2):
    x = np.array([x1, x2])
    
    # x1ê³¼ x2ì— ê°ê° ê³±í•´ì¤„ ê°€ì¤‘ì¹˜ 0.5, 0.5ë¡œ ì„¤ì •
    weight = np.array([0.5,0.5])
    
    # 1-1. AND gateë¥¼ ë§Œì¡±í•˜ëŠ” biasë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    bias = -0.7
    
    # 1-2. ê°€ì¤‘ì¹˜, ì…ë ¥ê°’, í¸í–¥ì„ ì´ìš©í•˜ì—¬ ê°€ì¤‘ ì‹ í˜¸ì˜ ì´í•©ì„ êµ¬í•©ë‹ˆë‹¤.
    y = np.sum(x * weight) + bias
    
    # Step Function í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ AND gateì˜ ì¶œë ¥ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return Step_Function(y)

# 2. OR gate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
def OR_gate(x1, x2):
    x = np.array([x1, x2])
    
    # x1ê³¼ x2ì— ê°ê° ê³±í•´ì¤„ ê°€ì¤‘ì¹˜ 0.5, 0.5ë¡œ ì„¤ì •
    weight = np.array([0.5,0.5])
    
    # 2-1. OR gateë¥¼ ë§Œì¡±í•˜ëŠ” biasë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    bias = 0
    
    # 2-2. ê°€ì¤‘ì¹˜, ì…ë ¥ê°’, í¸í–¥ì„ ì´ìš©í•˜ì—¬ ê°€ì¤‘ ì‹ í˜¸ì˜ ì´í•©ì„ êµ¬í•©ë‹ˆë‹¤.
    y = np.sum(x * weight) + bias
    
    #Step Function í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ AND gateì˜ ì¶œë ¥ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    return Step_Function(y)

# 3. Step Function êµ¬í˜„
def Step_Function(y):
    if y > 0:
        return 1
    else:
        return 0  
    
def main():
    
    # AND Gateì™€ OR Gateì— ë„£ì–´ì¤„ Input ì…ë‹ˆë‹¤.
    array = np.array([[0,0], [0,1], [1,0], [1,1]])
    
    # AND Gateë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ ì¶œë ¥í•˜ì—¬ í™•ì¸í•©ë‹ˆë‹¤.
    print('AND Gate ì¶œë ¥')
    for x1, x2 in array:
        print('Input: ',x1, x2, ', Output: ',AND_gate(x1, x2))
        
    # OR Gateë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ ì¶œë ¥í•˜ì—¬ í™•ì¸í•©ë‹ˆë‹¤.
    print('\nOR Gate ì¶œë ¥')
    for x1, x2 in array:
        print('Input: ',x1, x2, ', Output: ',OR_gate(x1, x2))

if __name__ == "__main__":
    main()
```

### ì‹¤ìŠµ-NAND, NOR

```python
import numpy as np
from elice_utils import EliceUtils

elice_utils = EliceUtils()

# 1. NAND_gate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
def NAND_gate(x1, x2):
    x = np.array([x1, x2])
    weight = np.array([-0.5, -0.5])
    bias = 0.7
    y = np.sum(x * weight) + bias
    
    return Step_Function(y)
    
# 2. NOR gate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
def NOR_gate(x1, x2):
    x = np.array([x1, x2])
    weight = np.array([-0.5, -0.5])
    bias = +0.3
    y = np.sum(x * weight) + bias
    
    return Step_Function(y) 
    
# 3. Step Function í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
# ì• ì‹¤ìŠµì—ì„œ êµ¬í˜„í•œ í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
def Step_Function(y):
    return 1 if y > 0 else 0
    
def main():
    
    # NAND, NOR Gateì— ë„£ì–´ì¤„ Input
    array = np.array([[0,0], [0,1], [1,0], [1,1]])
    
    # NAND, NOR Gateë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ ì¶œë ¥í•˜ì—¬ í™•ì¸
    print('NAND Gate ì¶œë ¥')
    for x1, x2 in array:
        print('Input: ',x1, x2, ' Output: ',NAND_gate(x1, x2))
        
    print('NOR Gate ì¶œë ¥')
    for x1, x2 in array:
        print('Input: ',x1, x2, ' Output: ',NOR_gate(x1, x2))
        
if __name__ == "__main__":
    main()
```


## ğŸ“Œ **4. ë¹„ì„ í˜•ì ì¸ ë¬¸ì œ**

* í•˜ë‚˜ì˜ ì„ ë§Œìœ¼ë¡œëŠ” ë¶„ë¥˜ê°€ ë¶ˆê°€ëŠ¥í•œ ë¬¸ì œê°€ ìˆìŒ
* ì—¬ëŸ¬ê°œë¥¼ ìŒ“ì•„ì„œ í•´ê²°í•œë‹¤!
* XOR

### XOR êµ¬í˜„í•˜ê¸°
```python
import numpy as np
from elice_utils import EliceUtils

elice_utils = EliceUtils()

# 1. `AND_gate` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”. 
def AND_gate(x1,x2):
    x = np.array([x1, x2])
    weight = np.array([0.5, 0.5])
    bias = -0.7
    y = np.sum(x * weight) + bias
    return Step_Function(y)
    
# 2. `OR_gate` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
def OR_gate(x1,x2):
    x = np.array([x1, x2])
    weight = np.array([0.5, 0.5])
    bias = -0.3
    y = np.sum(x * weight) + bias
    return Step_Function(y)
    
# 3. `NAND_gate` í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
def NAND_gate(x1,x2):
    x = np.array([x1, x2])
    weight = np.array([-0.5, -0.5])
    bias = 0.7
    y = np.sum(x * weight) + bias
    return Step_Function(y)
    
# 4. Step_Function í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”.
def Step_Function(y):
    return 1 if y > 0 else 0
    
# 5. êµ¬í˜„í•œ AND, OR, NAND gate í•¨ìˆ˜ë“¤ì„ í™œìš©í•˜ì—¬ XOR_gate í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”. 
def XOR_gate(x1, x2):
    A = NAND_gate(x1, x2)
    B = OR_gate(x1, x2)
    Q = AND_gate(A, B)
    
    y = Q
    
    return y
    

def main():
    # NOR gateì— ë„£ì–´ì¤„ Input
    array = np.array([[0,0], [0,1], [1,0], [1,1]])
    
    # XOR gateë¥¼ ë§Œì¡±í•˜ëŠ”ì§€ ì¶œë ¥í•˜ì—¬ í™•ì¸
    print('XOR Gate ì¶œë ¥')
    for x1, x2 in array:
        print('Input: ',x1, x2, ', Output: ', NAND_gate(x1, x2))


if __name__ == "__main__":
    main()
```

## ğŸ“Œ **5. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)**

* ì…ë ¥ì¸µ, íˆë“ ì¸µ, ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±
* hidden layerê°€ 1ì¸µì¼ ê²½ìš° ì„ í˜•ë¶„ë¦¬, 2ì¸µì€ êµ¬ì—­ë¶„ë¦¬, Nì¸µì€ ì„¸ë¶„í™”ëœ ë¶„ë¦¬ ê°€ëŠ¥